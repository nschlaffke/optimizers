{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "optimizers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "environment": {
      "name": "pytorch-gpu.1-4.m46",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m46"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MTySIGdS5xYW"
      },
      "source": [
        "Anti disconnect\n",
        "```javascript\n",
        "function ConnectButton(){\n",
        "    console.log(\"Connect pushed\"); \n",
        "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click() \n",
        "}\n",
        "setInterval(ConnectButton, 60000);\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LIrypeNcu2Zx"
      },
      "source": [
        "# Introduction\n",
        "## Algorithms\n",
        "Considered algorithms:\n",
        "* SGD + momentum\n",
        "* Adam\n",
        "* [AdamW](https://arxiv.org/abs/1711.05101)\n",
        "* [AdaBound](https://arxiv.org/abs/1902.09843)\n",
        "* [Radam](https://arxiv.org/abs/1908.03265)\n",
        "* [COCOB](https://arxiv.org/abs/1705.07795)\n",
        "* [Storm](https://arxiv.org/abs/1905.10018) (**to be implemented**)\n",
        "\n",
        "If time allows:\n",
        "* [Hypergradient Descent](https://arxiv.org/abs/1703.04782)\n",
        "* [Lookahead](https://arxiv.org/abs/1907.08610)\n",
        "\n",
        "Considered problems/dataset:\n",
        "* Image recognition:\n",
        "    * CIFAR-10\n",
        "    * CIFAR-100\n",
        "    * Street View House Number\n",
        "* Language Modelling:\n",
        "  * PenTreebank\n",
        "* Machine translation (maybe later):\n",
        "  * ?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VDUVmwKQXuVA"
      },
      "source": [
        "## Package installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XoKGlfiCu5go",
        "outputId": "fc8f2fa4-9c21-4963-d201-ff77982b26c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "!pip install -q adabound\n",
        "!pip install -q git+https://github.com/gbaydin/hypergradient-descent.git\n",
        "!pip install -q lookahead\n",
        "!pip install -q wandb"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for hypergrad (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for lookahead (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 2.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 8.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 460kB 16.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 9.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 19.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 7.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 7.2MB/s \n",
            "\u001b[?25h  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DqJTkrPRXs_D"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V3-2ls7x3uMl",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import transforms, models, datasets\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import adabound\n",
        "\n",
        "import wandb\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.style as style \n",
        "import seaborn as sns\n",
        "style.use('seaborn-poster') \n",
        "style.use('dark_background')\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XLz1ObNv8ent"
      },
      "source": [
        "## Optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RQ99dgnu8giQ"
      },
      "source": [
        "### Cocob"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jIsP9jN08eQ1",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "import torch\n",
        "\n",
        "###########################################################################\n",
        "# Training Deep Networks without Learning Rates Through Coin Betting\n",
        "# Paper: https://arxiv.org/abs/1705.07795\n",
        "#\n",
        "# NOTE: This optimizer is hardcoded to run on GPU, needs to be parametrized\n",
        "###########################################################################\n",
        "\n",
        "class COCOBBackprop(optim.Optimizer):\n",
        "\n",
        "    def __init__(self, params, alpha=100, epsilon=1e-8, weight_decay=0):\n",
        "\n",
        "        self._alpha = alpha\n",
        "        self.epsilon = epsilon\n",
        "        defaults = dict(alpha=alpha, epsilon=epsilon, weight_decay=weight_decay)\n",
        "        super(COCOBBackprop, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                grad = p.grad.data\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    grad = grad.add(p.data, alpha=group['weight_decay'])\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state['gradients_sum'] = torch.zeros_like(p.data).cuda().float()\n",
        "                    state['grad_norm_sum'] = torch.zeros_like(p.data).cuda().float()\n",
        "                    state['L'] = self.epsilon * torch.ones_like(p.data).cuda().float()\n",
        "                    state['tilde_w'] = torch.zeros_like(p.data).cuda().float()\n",
        "                    state['reward'] = torch.zeros_like(p.data).cuda().float()\n",
        "\n",
        "                gradients_sum = state['gradients_sum']\n",
        "                grad_norm_sum = state['grad_norm_sum']\n",
        "                tilde_w = state['tilde_w']\n",
        "                L = state['L']\n",
        "                reward = state['reward']\n",
        "\n",
        "                zero = torch.cuda.FloatTensor([0.])\n",
        "\n",
        "                L_update = torch.max(L, torch.abs(grad))\n",
        "                gradients_sum_update = gradients_sum + grad\n",
        "                grad_norm_sum_update = grad_norm_sum + torch.abs(grad)\n",
        "                reward_update = torch.max(reward - grad * tilde_w, zero)\n",
        "                new_w = -gradients_sum_update/(L_update * (torch.max(grad_norm_sum_update + L_update, self._alpha * L_update)))*(reward_update + L_update)\n",
        "                p.data = p.data - tilde_w + new_w\n",
        "                tilde_w_update = new_w\n",
        "\n",
        "                state['gradients_sum'] = gradients_sum_update\n",
        "                state['grad_norm_sum'] = grad_norm_sum_update\n",
        "                state['L'] = L_update\n",
        "                state['tilde_w'] = tilde_w_update\n",
        "                state['reward'] = reward_update\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "buXLcHlGxa8i"
      },
      "source": [
        "### Radam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X2Xt-8tGxfxB",
        "colab": {}
      },
      "source": [
        "# Source: https://github.com/LiyuanLucasLiu/RAdam/tree/master/radam\n",
        "import math\n",
        "import torch\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "\n",
        "class RAdam(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=True):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "        \n",
        "        self.degenerated_to_sgd = degenerated_to_sgd\n",
        "        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n",
        "            for param in params:\n",
        "                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n",
        "                    param['buffer'] = [[None, None, None] for _ in range(10)]\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, buffer=[[None, None, None] for _ in range(10)])\n",
        "        super(RAdam, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(RAdam, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
        "                else:\n",
        "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "\n",
        "                state['step'] += 1\n",
        "                buffered = group['buffer'][int(state['step'] % 10)]\n",
        "                if state['step'] == buffered[0]:\n",
        "                    N_sma, step_size = buffered[1], buffered[2]\n",
        "                else:\n",
        "                    buffered[0] = state['step']\n",
        "                    beta2_t = beta2 ** state['step']\n",
        "                    N_sma_max = 2 / (1 - beta2) - 1\n",
        "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
        "                    buffered[1] = N_sma\n",
        "\n",
        "                    # more conservative since it's an approximated value\n",
        "                    if N_sma >= 5:\n",
        "                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
        "                    elif self.degenerated_to_sgd:\n",
        "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
        "                    else:\n",
        "                        step_size = -1\n",
        "                    buffered[2] = step_size\n",
        "\n",
        "                # more conservative since it's an approximated value\n",
        "                if N_sma >= 5:\n",
        "                    if group['weight_decay'] != 0:\n",
        "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
        "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                    p_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg, denom)\n",
        "                    p.data.copy_(p_data_fp32)\n",
        "                elif step_size > 0:\n",
        "                    if group['weight_decay'] != 0:\n",
        "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
        "                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n",
        "                    p.data.copy_(p_data_fp32)\n",
        "\n",
        "        return loss\n",
        "\n",
        "class PlainRAdam(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=True):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "                    \n",
        "        self.degenerated_to_sgd = degenerated_to_sgd\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
        "\n",
        "        super(PlainRAdam, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(PlainRAdam, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
        "                else:\n",
        "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "\n",
        "                state['step'] += 1\n",
        "                beta2_t = beta2 ** state['step']\n",
        "                N_sma_max = 2 / (1 - beta2) - 1\n",
        "                N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
        "\n",
        "\n",
        "                # more conservative since it's an approximated value\n",
        "                if N_sma >= 5:\n",
        "                    if group['weight_decay'] != 0:\n",
        "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
        "                    step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
        "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
        "                    p.data.copy_(p_data_fp32)\n",
        "                elif self.degenerated_to_sgd:\n",
        "                    if group['weight_decay'] != 0:\n",
        "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
        "                    step_size = group['lr'] / (1 - beta1 ** state['step'])\n",
        "                    p_data_fp32.add_(-step_size, exp_avg)\n",
        "                    p.data.copy_(p_data_fp32)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class AdamW(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, warmup = 0):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "        \n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
        "                        weight_decay=weight_decay, warmup = warmup)\n",
        "        super(AdamW, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(AdamW, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
        "                else:\n",
        "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "\n",
        "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                bias_correction1 = 1 - beta1 ** state['step']\n",
        "                bias_correction2 = 1 - beta2 ** state['step']\n",
        "                \n",
        "                if group['warmup'] > state['step']:\n",
        "                    scheduled_lr = 1e-8 + state['step'] * group['lr'] / group['warmup']\n",
        "                else:\n",
        "                    scheduled_lr = group['lr']\n",
        "\n",
        "                step_size = scheduled_lr * math.sqrt(bias_correction2) / bias_correction1\n",
        "                \n",
        "                if group['weight_decay'] != 0:\n",
        "                    p_data_fp32.add_(-group['weight_decay'] * scheduled_lr, p_data_fp32)\n",
        "\n",
        "                p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
        "\n",
        "                p.data.copy_(p_data_fp32)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nSkDsEjZPN-x"
      },
      "source": [
        "## Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Vhti1gwcPQtm"
      },
      "source": [
        "### ResNet18/34/50/101/152 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mCaGkVqKOY6h",
        "colab": {}
      },
      "source": [
        "# Source: https://github.com/uoguelph-mlrg/Cutout/blob/master/model/resnet.py \n",
        "\n",
        "'''ResNet18/34/50/101/152 in Pytorch.'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = conv3x3(3,64)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18(num_classes=10):\n",
        "    return ResNet(BasicBlock, [2,2,2,2], num_classes)\n",
        "\n",
        "def ResNet34(num_classes=10):\n",
        "    return ResNet(BasicBlock, [3,4,6,3], num_classes)\n",
        "\n",
        "def ResNet50(num_classes=10):\n",
        "    return ResNet(Bottleneck, [3,4,6,3], num_classes)\n",
        "\n",
        "def ResNet101(num_classes=10):\n",
        "    return ResNet(Bottleneck, [3,4,23,3], num_classes)\n",
        "\n",
        "def ResNet152(num_classes=10):\n",
        "    return ResNet(Bottleneck, [3,8,36,3], num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gT1w8KsUNhme"
      },
      "source": [
        "## Helpers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fOXkj3XehFI1"
      },
      "source": [
        "Progress bar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GllCRIjjhC--",
        "colab": {}
      },
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def progress(value, max=100):\n",
        "    return HTML(\"\"\"\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 100%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(value=value, max=max))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a96rpTpgGeNc"
      },
      "source": [
        "Seeder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y2z5m0BAOWug",
        "colab": {}
      },
      "source": [
        "def random_state():\n",
        "  seed = np.random.randint(100000)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  np.random.seed(seed)\n",
        "  return seed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tU9gMRq9Nd6O"
      },
      "source": [
        "### DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "47XNehrVFQg0",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import torchvision.datasets\n",
        "import torchvision.transforms \n",
        "\n",
        "class DataLoader:\n",
        "\n",
        "  DATA_ROOT = './data'\n",
        "\n",
        "  def __init__(self, set_name, train_transform, test_transform, \n",
        "               batch_size_train=256, batch_size_val=1024,\n",
        "               shuffle=True, data_root='./data'):\n",
        "    \n",
        "    self.data_root = data_root\n",
        "    self.batch_size_train = batch_size_train\n",
        "    self.batch_size_val = batch_size_val\n",
        "    self.shuffle = shuffle\n",
        "    self.num_workers = os.cpu_count()\n",
        "    self.set_name = set_name\n",
        "    self._loader_params_train = self._get_loader_params(True)\n",
        "    self._loader_params_val = self._get_loader_params(False)\n",
        "    self._mean = None\n",
        "    self._std = None\n",
        "    self._train_transform = train_transform\n",
        "    self._test_transform = test_transform\n",
        "\n",
        "  def get_loaders(self):\n",
        "    trainloader = self.get_loader(True)\n",
        "    testloader = self.get_loader(False)\n",
        "    return trainloader, testloader\n",
        "\n",
        "  def get_loader(self, train):\n",
        "    params = self._loader_params_train if train else self._loader_params_val\n",
        "    transform = self._train_transform if train else self._test_transform\n",
        "    dataset_f = getattr(torchvision.datasets, self.set_name)\n",
        "    dataset = dataset_f(self.data_root, train=train, \n",
        "                        transform=transform, \n",
        "                        download=True)\n",
        "    return torch.utils.data.DataLoader(dataset, \n",
        "                                       **params)\n",
        "  \n",
        "  def _get_loader_params(self, train):\n",
        "    return {\n",
        "        'batch_size': self.batch_size_train if train else self.batch_size_val,\n",
        "        'shuffle': self.shuffle,\n",
        "        'num_workers': self.num_workers\n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Lk0df0GTPEoS"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jhUYOv6KPEM8",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torchvision\n",
        "\n",
        "class Utils:\n",
        "\n",
        "  @staticmethod\n",
        "  def imshow(img):\n",
        "      img = img / 2 + 0.5 # unnormalize\n",
        "      npimg = img.numpy()\n",
        "      plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "      plt.show()\n",
        "  \n",
        "  @staticmethod\n",
        "  def show_random_images(loader):\n",
        "    dataiter = iter(loader)\n",
        "    images, labels = dataiter.next()\n",
        "    Utils.imshow(torchvision.utils.make_grid(images))\n",
        "    # print labels\n",
        "    print(' '.join('%5s' % classes[labels[j]] for j in range(loader.batch_size)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mhjyZYN4YWBn"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h-gsceuTim1B",
        "colab": {}
      },
      "source": [
        "class Trainer:\n",
        "\n",
        "  HTML = False\n",
        "\n",
        "  def __init__(self, model, trainloader, testloader, \n",
        "               criterion, optimizer, num_epochs, \n",
        "               score=accuracy_score,\n",
        "               scheduler=None, wandb=None):\n",
        "    \n",
        "    if not(torch.cuda.is_available()) and not(CPU):\n",
        "      raise EnvironmentError('Turn on GPU acceleration')\n",
        "\n",
        "    self._device = torch.device('cuda:0')\n",
        "    self._model = model.to(self._device)\n",
        "    self._trainloader = trainloader\n",
        "    self._testloader = testloader\n",
        "    self._criterion = criterion\n",
        "    self._optimizer = optimizer\n",
        "    self._num_epochs = num_epochs\n",
        "    self._scheduler = scheduler\n",
        "    self._score = score\n",
        "\n",
        "    self._BATCH_SIZE = trainloader.batch_size\n",
        "    self._TRAIN_SIZE = trainloader.batch_size * len(trainloader)\n",
        "    self._TEST_SIZE = testloader.batch_size * len(testloader)\n",
        "\n",
        "    self._best_model_wts = copy.deepcopy(self._model.state_dict())\n",
        "    self._best_score = 0\n",
        "    self._epoch = 0\n",
        "    self._train_hist = self._get_hist_dict()\n",
        "    self._val_hist = self._get_hist_dict()\n",
        "\n",
        "    self._bar = None\n",
        "    self._wandb = wandb\n",
        "    self._wandb_entry = {}\n",
        "\n",
        "    self._best_scores = {'train': 0,\n",
        "                         'val': 0}\n",
        "    \n",
        "\n",
        "  def _get_hist_dict(self):\n",
        "    return {\n",
        "        'scores': np.zeros(self._num_epochs),\n",
        "        'losses': np.zeros(self._num_epochs)\n",
        "    }\n",
        "\n",
        "  def train(self):\n",
        "    for self._epoch in range(self._num_epochs):\n",
        "      self._log_epoch()\n",
        "\n",
        "      self._model.train()\n",
        "      self._pass()\n",
        "\n",
        "      self._model.eval()\n",
        "      self._pass()\n",
        "      self._scheduler_step()\n",
        "\n",
        "    self._save_model()\n",
        "      \n",
        "  def _save_model(self):\n",
        "    name = 'model_' + str(self._epoch) + '.pt'\n",
        "    torch.save({\n",
        "            'model_state_dict': self._model.state_dict(),\n",
        "            'optimizer_state_dict': self._optimizer.state_dict(),\n",
        "            }, os.path.join(wandb.run.dir, name))\n",
        "    wandb.save(name)\n",
        "\n",
        "  def _pass(self):\n",
        "    loader = self._get_loader()\n",
        "    losses = np.zeros(len(loader))\n",
        "    scores = np.zeros(len(loader))\n",
        "    self._show_bar()\n",
        "    for i, (X, y) in enumerate(loader):\n",
        "      X = X.to(self._device)\n",
        "      y = y.to(self._device)\n",
        "      with torch.set_grad_enabled(self._model.training):\n",
        "        self._optimizer.zero_grad()\n",
        "        y_pred = self._model(X)\n",
        "        _, preds = torch.max(y_pred, 1)\n",
        "        loss = self._criterion(y_pred, y)\n",
        "        losses[i] = loss.item()\n",
        "        scores[i] = self._score(y.tolist(), preds.tolist())\n",
        "        if self._model.training:\n",
        "          loss.backward()\n",
        "          self._optimizer.step()\n",
        "        self._update_bar(i+1, loss.item(), scores[i].item())\n",
        "    self._update_hist(np.mean(losses), np.mean(scores))\n",
        "  \n",
        "  def _update_hist(self, loss, score):\n",
        "    training = self._model.training\n",
        "    hist = self._train_hist if training else self._val_hist\n",
        "    hist['losses'][self._epoch] = loss\n",
        "    hist['scores'][self._epoch] = score\n",
        "\n",
        "    best_type = 'train' if training else 'val'\n",
        "    best = self._best_scores[best_type]\n",
        "    if best < score:\n",
        "      self._best_scores[best_type] = score\n",
        "      if self._wandb:\n",
        "        wandb.run.summary[\"best_\" + best_type + \"_acc\"] = score\n",
        "        wandb.run.summary.update()\n",
        "      \n",
        "    if self._wandb:\n",
        "      entry = self._wandb_entry\n",
        "      key = 'train' if training else 'val'\n",
        "      entry[key + '_loss'] = loss\n",
        "      entry[key + '_acc'] = score\n",
        "      if key=='val':\n",
        "        self._wandb.log(entry)\n",
        "        self._wandb_entry = {}\n",
        "        wandb.run.summary[\"finished_epochs\"] = self._epoch\n",
        "        wandb.run.summary.update()\n",
        "\n",
        "  def _scheduler_step(self):\n",
        "    if self._scheduler:\n",
        "      self._scheduler.step()\n",
        "  \n",
        "  def _get_loader(self):\n",
        "    if not(self._model.training):\n",
        "      return self._testloader\n",
        "    return self._trainloader\n",
        "\n",
        "  def _log_epoch(self):\n",
        "      print('Epoch {}/{}'.format(self._epoch, self._num_epochs - 1))\n",
        "      print('-' * 10)\n",
        "\n",
        "  def _show_bar(self):\n",
        "    html = self._get_html_bar(0, 0)\n",
        "    self._bar = display(html, display_id=True)\n",
        "\n",
        "  def _get_html_bar(self, value, loss=0, score=0):\n",
        "    max = len(self._trainloader) if \\\n",
        "     self._model.training else len(self._testloader)\n",
        "    prefix = 'train_' if self._model.training else 'val_'\n",
        "\n",
        "    return HTML(\"\"\"\n",
        "                <progress\n",
        "                    value='{value}'\n",
        "                    max='{max}',\n",
        "                    style='width: 60%',\n",
        "                >\n",
        "                    {value}\n",
        "                </progress> \n",
        "                <span style='margin-left: 10px'>\n",
        "                  {prefix}loss: {loss:.3f} {prefix}acc: {score:.3f}\n",
        "                <span>\n",
        "              \"\"\".format(value=value, max=max, \n",
        "                         loss=loss, score=score,\n",
        "                         prefix=prefix))\n",
        "  \n",
        "  def _update_bar(self, value, loss, score):\n",
        "    if not(self._bar):\n",
        "      raise RuntimeError('Bar not being shown')\n",
        "    self._bar.update(self._get_html_bar(value, loss, score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6UwlxngUPiqU"
      },
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tOZVk0-bvBQR",
        "colab": {}
      },
      "source": [
        "class OptimizerProvider:\n",
        "  OPTIMIZERS = ['SGD', 'Adam', 'AdamW', 'Cocob', 'Radam', 'Adabound']\n",
        "\n",
        "  @staticmethod\n",
        "  def get(opt_name, model_parameters, params):\n",
        "    if opt_name not in OptimizerProvider.OPTIMIZERS:\n",
        "      raise RuntimeError('Unknown optimizer')\n",
        "\n",
        "    lr = params['lr']\n",
        "    wd = params['weight_decay']\n",
        "\n",
        "    if opt_name == 'Adam':\n",
        "      return optim.Adam(model_parameters,\n",
        "                        lr=lr,\n",
        "                        weight_decay=wd)\n",
        "\n",
        "    elif opt_name == 'AdamW':\n",
        "      return optim.AdamW(model_parameters,\n",
        "                         lr=lr,\n",
        "                         weight_decay=wd)\n",
        "\n",
        "    elif opt_name == 'Radam':\n",
        "      return RAdam(model_parameters,\n",
        "                   lr=lr,\n",
        "                   weight_decay=wd)\n",
        "\n",
        "    elif opt_name == 'SGD': \n",
        "      return optim.SGD(model_parameters,\n",
        "                       lr=lr,\n",
        "                       weight_decay=wd)\n",
        "\n",
        "    elif opt_name == 'Cocob': \n",
        "      return COCOBBackprop(model_parameters,\n",
        "                           weight_decay=wd)\n",
        "\n",
        "    elif opt_name == 'Adabound': \n",
        "      return adabound.AdaBound(model_parameters,\n",
        "                               lr=lr,\n",
        "                               weight_decay=wd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ioGHMiXf-Jqe"
      },
      "source": [
        "### CIFAR-10 / CIFAR-100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xesh7Ug26s67"
      },
      "source": [
        "Normalization and augmentation the same for **Cifar-10** and **Cifar-100** and taken from  \n",
        "[https://github.com/uoguelph-mlrg/Cutout](https://github.com/uoguelph-mlrg/Cutout)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KqtpK4Dmt_xL",
        "colab": {}
      },
      "source": [
        "class CifarExperiment:\n",
        "\n",
        "  BATCH_SIZE = 128\n",
        "  EPOCHS = 200\n",
        "  CIFAR_10 = 'CIFAR10'\n",
        "  CIFAR_100 = 'CIFAR100'\n",
        "  CIFAR_10_PROJECT = 'ms-cifar10-resNet18-v2'\n",
        "  CIFAR_100_PROJECT = 'ms-cifar100-resNet18'\n",
        "\n",
        "  @staticmethod\n",
        "  def train_resNet18_cifar10(opt_name, **params):\n",
        "    nclasses = 10\n",
        "    CifarExperiment._train_resNet18_cifar(\n",
        "                                         CifarExperiment.CIFAR_10,\n",
        "                                         nclasses, \n",
        "                                         CifarExperiment.CIFAR_10_PROJECT,\n",
        "                                         opt_name,\n",
        "                                         **params)\n",
        "\n",
        "  @staticmethod\n",
        "  def train_resNet18_cifar100(opt_name, **params):\n",
        "    nclasses = 100\n",
        "    CifarExperiment._train_resNet18_cifar(\n",
        "                                         CifarExperiment.CIFAR_100, \n",
        "                                         nclasses,\n",
        "                                         CifarExperiment.CIFAR_100_PROJECT,\n",
        "                                         opt_name,\n",
        "                                         **params)\n",
        "\n",
        "  @staticmethod\n",
        "  def _get_transforms():\n",
        "    normalize = transforms.Normalize(mean=[x/255.0 for x in [125.3, 123.0, 113.9]],\n",
        "                                  std=[x/255.0 for x in [63.0, 62.1, 66.7]])\n",
        "\n",
        "    train_transform = transforms.Compose([])\n",
        "    train_transform.transforms.append(transforms.RandomCrop(32, padding=4))\n",
        "    train_transform.transforms.append(transforms.RandomHorizontalFlip())\n",
        "    train_transform.transforms.append(transforms.ToTensor())\n",
        "    train_transform.transforms.append(normalize)\n",
        "\n",
        "    test_transform = transforms.Compose([])\n",
        "    test_transform.transforms.append(transforms.ToTensor())\n",
        "    test_transform.transforms.append(normalize)\n",
        "\n",
        "    return train_transform, test_transform\n",
        "  \n",
        "  @staticmethod\n",
        "  def _train_resNet18_cifar(dataset, nclasses, project_name, opt_name, lrs, weight_decays, tags=[]):\n",
        "\n",
        "    # LR step decay\n",
        "    # src: \"Lookahead Optimizer: k steps forward, 1 step back\" - Hintot et al.\n",
        "    STEP_SIZE = 50\n",
        "    GAMMA = 1/5\n",
        "    \n",
        "    train_transform, test_transform = CifarExperiment._get_transforms()\n",
        "\n",
        "    parameters = {\n",
        "        'lr': lrs,\n",
        "        'batch_size': [CifarExperiment.BATCH_SIZE],\n",
        "        'epochs': [CifarExperiment.EPOCHS],\n",
        "        'weight_decay': weight_decays,\n",
        "        'lr_decay': ['StepLR'],\n",
        "        'init': ['default'],\n",
        "        'optimizer': [opt_name]\n",
        "    }\n",
        "\n",
        "    grid = ParameterGrid(parameters)\n",
        "\n",
        "    trainloader, testloader = DataLoader(dataset, \n",
        "                                        train_transform, test_transform,\n",
        "                                        batch_size_train=CifarExperiment.BATCH_SIZE).get_loaders() \n",
        "    # Grid search \n",
        "    for config in grid:\n",
        "      seed = random_state()\n",
        "      config['seed'] = seed\n",
        "      \n",
        "      wandb.init(reinit=True,\n",
        "                project=project_name,\n",
        "                tags=tags,\n",
        "                config=config)\n",
        "      \n",
        "      model = ResNet18(nclasses)\n",
        "      optimizer = OptimizerProvider.get(opt_name, model.parameters(), config) \n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "      # lr decay\n",
        "      if opt_name in ['Cocob', 'Adabound']:\n",
        "          scheduler = None\n",
        "      else:\n",
        "          scheduler = StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
        "\n",
        "      # Training\n",
        "      trainer = Trainer(model, trainloader, testloader, \n",
        "                        criterion, optimizer, CifarExperiment.EPOCHS, \n",
        "                        wandb=wandb, scheduler=scheduler)\n",
        "      wandb.watch(model)\n",
        "      trainer.train()\n",
        "      wandb.join()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rMpBhglU5Mi0"
      },
      "source": [
        "_wandb_ token:  \n",
        "`XYZ`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by46x_Ro_WE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wandb login"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JpMifrx61rY7",
        "colab": {}
      },
      "source": [
        "lrs = [0.1]\n",
        "weight_decays = [0.01]\n",
        "CifarExperiment.train_resNet18_cifar10('SGD', lrs=lrs, weight_decays=weight_decays)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}